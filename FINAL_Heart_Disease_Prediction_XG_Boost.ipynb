{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "APXtC2Eze_5K",
        "outputId": "d73413fa-4243-4d65-9afe-dec9cfd82248"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8b3ca088-4559-4511-aa0e-796703dd9bf6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8b3ca088-4559-4511-aa0e-796703dd9bf6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Heart_Disease_Prediction .xlsx to Heart_Disease_Prediction .xlsx\n",
            "Uploaded file: Heart_Disease_Prediction .xlsx\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# This line is used to upload files from your system to the Colab environment\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Display the list of uploaded files\n",
        "for filename in uploaded.keys():\n",
        "    print(f'Uploaded file: {filename}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Load the data from an Excel file\n",
        "file_path = 'Heart_Disease_Prediction .xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Set the target and features\n",
        "X = data.drop('Heart Disease', axis=1)  # Features\n",
        "y = data['Heart Disease']  # Target\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Feature selection using SelectKBest\n",
        "selector = SelectKBest(f_classif, k=min(8, X_train_scaled.shape[1]))  # Maximum of 8 features\n",
        "X_train_scaled = selector.fit_transform(X_train_scaled, y_train)\n",
        "X_val_scaled = selector.transform(X_val_scaled)\n",
        "X_test_scaled = selector.transform(X_test_scaled)\n",
        "\n",
        "# Now the data is ready for model training\n",
        "print(\"Train data shape:\", X_train_scaled.shape)\n",
        "print(\"Validation data shape:\", X_val_scaled.shape)\n",
        "print(\"Test data shape:\", X_test_scaled.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab988qaCqC2p",
        "outputId": "a0c8a8f2-78b9-4cb2-a7ff-9243312cb5ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape: (189, 8)\n",
            "Validation data shape: (40, 8)\n",
            "Test data shape: (41, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Upload the dataset in Colab (uncomment and run in Colab)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# Load the dataset\n",
        "# Replace 'Heart_Disease_Prediction.xlsx' with the exact file name if different\n",
        "df = pd.read_excel('Heart_Disease_Prediction .xlsx')\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "# Encode target variable\n",
        "df['Heart Disease'] = df['Heart Disease'].map({'Presence': 1, 'Absence': 0})\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop('Heart Disease', axis=1)\n",
        "y = df['Heart Disease']\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['Sex', 'Chest pain type', 'FBS over 120', 'EKG results',\n",
        "                    'Exercise angina', 'Slope of ST', 'Number of vessels fluro', 'Thallium']\n",
        "numerical_cols = ['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression']\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Step 2: Split the data\n",
        "# Train (70%), Validation (15%), Test (15%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp)\n",
        "# Note: 0.1765 = 0.15/(1-0.15) to get 15% of total as validation\n",
        "\n",
        "# Step 3: Create XGBoost model pipeline\n",
        "xgb = XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', xgb)\n",
        "])\n",
        "\n",
        "# Step 4: Hyperparameter tuning\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 150, 200],\n",
        "    'classifier__max_depth': [3, 5, 7],\n",
        "    'classifier__learning_rate': [0.01, 0.1, 0.3],\n",
        "    'classifier__subsample': [0.8, 1.0],\n",
        "    'classifier__colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "def evaluate_model(model, X_set, y_set, set_name):\n",
        "    y_pred = model.predict(X_set)\n",
        "    accuracy = accuracy_score(y_set, y_pred)\n",
        "    precision = precision_score(y_set, y_pred)\n",
        "    recall = recall_score(y_set, y_pred)\n",
        "    f1 = f1_score(y_set, y_pred)\n",
        "    print(f\"\\n{set_name} Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"Classification Report:\\n{classification_report(y_set, y_pred)}\")\n",
        "\n",
        "# Evaluate on training, validation, and test sets\n",
        "evaluate_model(best_model, X_train, y_train, \"Training\")\n",
        "evaluate_model(best_model, X_val, y_val, \"Validation\")\n",
        "evaluate_model(best_model, X_test, y_test, \"Test\")\n",
        "\n",
        "# Step 6: Feature importance (optional)\n",
        "# Extract feature names after one-hot encoding\n",
        "cat_transformer = best_model.named_steps['preprocessor'].named_transformers_['cat']\n",
        "cat_feature_names = cat_transformer.get_feature_names_out(categorical_cols)\n",
        "feature_names = numerical_cols + list(cat_feature_names)\n",
        "\n",
        "# Get feature importances\n",
        "importances = best_model.named_steps['classifier'].feature_importances_\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance.sort_values(by='Importance', ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M2A2sttliSw",
        "outputId": "f3050dbb-a5ca-48ad-e5e7-865398b79ecb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
            "Best Parameters: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 100, 'classifier__subsample': 0.8}\n",
            "\n",
            "Training Metrics:\n",
            "Accuracy: 0.87\n",
            "Precision: 0.94\n",
            "Recall: 0.76\n",
            "F1-Score: 0.84\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.96      0.89       104\n",
            "           1       0.94      0.76      0.84        84\n",
            "\n",
            "    accuracy                           0.87       188\n",
            "   macro avg       0.89      0.86      0.87       188\n",
            "weighted avg       0.88      0.87      0.87       188\n",
            "\n",
            "\n",
            "Validation Metrics:\n",
            "Accuracy: 0.90\n",
            "Precision: 0.89\n",
            "Recall: 0.89\n",
            "F1-Score: 0.89\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91        23\n",
            "           1       0.89      0.89      0.89        18\n",
            "\n",
            "    accuracy                           0.90        41\n",
            "   macro avg       0.90      0.90      0.90        41\n",
            "weighted avg       0.90      0.90      0.90        41\n",
            "\n",
            "\n",
            "Test Metrics:\n",
            "Accuracy: 0.88\n",
            "Precision: 0.84\n",
            "Recall: 0.89\n",
            "F1-Score: 0.86\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.87      0.89        23\n",
            "           1       0.84      0.89      0.86        18\n",
            "\n",
            "    accuracy                           0.88        41\n",
            "   macro avg       0.88      0.88      0.88        41\n",
            "weighted avg       0.88      0.88      0.88        41\n",
            "\n",
            "\n",
            "Feature Importance:\n",
            "                      Feature  Importance\n",
            "19                 Thallium_7    0.148285\n",
            "8           Chest pain type_4    0.109964\n",
            "4               ST depression    0.084659\n",
            "12          Exercise angina_1    0.073159\n",
            "17  Number of vessels fluro_3    0.064997\n",
            "13              Slope of ST_2    0.064712\n",
            "5                       Sex_1    0.064276\n",
            "16  Number of vessels fluro_2    0.061347\n",
            "0                         Age    0.051643\n",
            "3                      Max HR    0.050933\n",
            "15  Number of vessels fluro_1    0.048986\n",
            "7           Chest pain type_3    0.044354\n",
            "11              EKG results_2    0.032656\n",
            "1                          BP    0.032095\n",
            "2                 Cholesterol    0.029887\n",
            "14              Slope of ST_3    0.027824\n",
            "18                 Thallium_6    0.005336\n",
            "9              FBS over 120_1    0.004888\n",
            "10              EKG results_1    0.000000\n",
            "6           Chest pain type_2    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the dataset\n",
        "# Replace 'Heart_Disease_Prediction.xlsx' with the exact file name if different\n",
        "df = pd.read_excel('Heart_Disease_Prediction .xlsx')\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "# Encode target variable\n",
        "df['Heart Disease'] = df['Heart Disease'].map({'Presence': 1, 'Absence': 0})\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop('Heart Disease', axis=1)\n",
        "y = df['Heart Disease']\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['Sex', 'Chest pain type', 'FBS over 120', 'EKG results',\n",
        "                    'Exercise angina', 'Slope of ST', 'Number of vessels fluro', 'Thallium']\n",
        "numerical_cols = ['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression']\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Step 2: Split the data\n",
        "# Train (70%), Validation (15%), Test (15%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp)\n",
        "# Note: 0.1765 = 0.15/(1-0.15) to get 15% of total as validation\n",
        "\n",
        "# Step 3: Create XGBoost model pipeline\n",
        "xgb = XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', xgb)\n",
        "])\n",
        "\n",
        "# Step 4: Hyperparameter tuning\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 150, 200],\n",
        "    'classifier__max_depth': [3, 5, 7],\n",
        "    'classifier__learning_rate': [0.01, 0.1, 0.3],\n",
        "    'classifier__subsample': [0.8, 1.0],\n",
        "    'classifier__colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "def evaluate_model(model, X_set, y_set, set_name):\n",
        "    y_pred = model.predict(X_set)\n",
        "    accuracy = accuracy_score(y_set, y_pred)\n",
        "    precision = precision_score(y_set, y_pred)\n",
        "    recall = recall_score(y_set, y_pred)\n",
        "    f1 = f1_score(y_set, y_pred)\n",
        "    print(f\"\\n{set_name} Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"Classification Report:\\n{classification_report(y_set, y_pred)}\")\n",
        "\n",
        "# Evaluate on training, validation, and test sets\n",
        "evaluate_model(best_model, X_train, y_train, \"Training\")\n",
        "evaluate_model(best_model, X_val, y_val, \"Validation\")\n",
        "evaluate_model(best_model, X_test, y_test, \"Test\")\n",
        "\n",
        "# Step 6: Feature importance (optional)\n",
        "# Extract feature names after one-hot encoding\n",
        "cat_transformer = best_model.named_steps['preprocessor'].named_transformers_['cat']\n",
        "cat_feature_names = cat_transformer.get_feature_names_out(categorical_cols)\n",
        "feature_names = numerical_cols + list(cat_feature_names)\n",
        "\n",
        "# Get feature importances\n",
        "importances = best_model.named_steps['classifier'].feature_importances_\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance.sort_values(by='Importance', ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzsVM87nwrxj",
        "outputId": "7afac0aa-e76d-4fd2-aa57-86b9e4d47079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
            "Best Parameters: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 100, 'classifier__subsample': 0.8}\n",
            "\n",
            "Training Metrics:\n",
            "Accuracy: 0.87\n",
            "Precision: 0.94\n",
            "Recall: 0.76\n",
            "F1-Score: 0.84\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.96      0.89       104\n",
            "           1       0.94      0.76      0.84        84\n",
            "\n",
            "    accuracy                           0.87       188\n",
            "   macro avg       0.89      0.86      0.87       188\n",
            "weighted avg       0.88      0.87      0.87       188\n",
            "\n",
            "\n",
            "Validation Metrics:\n",
            "Accuracy: 0.90\n",
            "Precision: 0.89\n",
            "Recall: 0.89\n",
            "F1-Score: 0.89\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91        23\n",
            "           1       0.89      0.89      0.89        18\n",
            "\n",
            "    accuracy                           0.90        41\n",
            "   macro avg       0.90      0.90      0.90        41\n",
            "weighted avg       0.90      0.90      0.90        41\n",
            "\n",
            "\n",
            "Test Metrics:\n",
            "Accuracy: 0.88\n",
            "Precision: 0.84\n",
            "Recall: 0.89\n",
            "F1-Score: 0.86\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.87      0.89        23\n",
            "           1       0.84      0.89      0.86        18\n",
            "\n",
            "    accuracy                           0.88        41\n",
            "   macro avg       0.88      0.88      0.88        41\n",
            "weighted avg       0.88      0.88      0.88        41\n",
            "\n",
            "\n",
            "Feature Importance:\n",
            "                      Feature  Importance\n",
            "19                 Thallium_7    0.148285\n",
            "8           Chest pain type_4    0.109964\n",
            "4               ST depression    0.084659\n",
            "12          Exercise angina_1    0.073159\n",
            "17  Number of vessels fluro_3    0.064997\n",
            "13              Slope of ST_2    0.064712\n",
            "5                       Sex_1    0.064276\n",
            "16  Number of vessels fluro_2    0.061347\n",
            "0                         Age    0.051643\n",
            "3                      Max HR    0.050933\n",
            "15  Number of vessels fluro_1    0.048986\n",
            "7           Chest pain type_3    0.044354\n",
            "11              EKG results_2    0.032656\n",
            "1                          BP    0.032095\n",
            "2                 Cholesterol    0.029887\n",
            "14              Slope of ST_3    0.027824\n",
            "18                 Thallium_6    0.005336\n",
            "9              FBS over 120_1    0.004888\n",
            "10              EKG results_1    0.000000\n",
            "6           Chest pain type_2    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the dataset (already uploaded in Colab)\n",
        "df = pd.read_excel('Heart_Disease_Prediction .xlsx')\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "# Encode target variable\n",
        "df['Heart Disease'] = df['Heart Disease'].map({'Presence': 1, 'Absence': 0})\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop('Heart Disease', axis=1)\n",
        "y = df['Heart Disease']\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['Sex', 'Chest pain type', 'FBS over 120', 'EKG results',\n",
        "                    'Exercise angina', 'Slope of ST', 'Number of vessels fluro', 'Thallium']\n",
        "numerical_cols = ['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression']\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Step 2: Split the data\n",
        "# Train (70%), Validation (15%), Test (15%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Step 3: Create pipeline with SMOTE and XGBoost\n",
        "xgb = XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "pipeline = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', xgb)\n",
        "])\n",
        "\n",
        "# Step 4: Expanded hyperparameter tuning\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 150, 200, 300],\n",
        "    'classifier__max_depth': [3, 4, 5],\n",
        "    'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
        "    'classifier__subsample': [0.7, 0.8, 0.9],\n",
        "    'classifier__colsample_bytree': [0.7, 0.8, 0.9],\n",
        "    'classifier__scale_pos_weight': [1, 1.2, 1.5]  # Adjust for class imbalance\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Step 5: Threshold tuning on validation set\n",
        "def evaluate_model_with_threshold(model, X_set, y_set, set_name, threshold=0.5):\n",
        "    y_scores = model.predict_proba(X_set)[:, 1]\n",
        "    y_pred = (y_scores >= threshold).astype(int)\n",
        "    accuracy = accuracy_score(y_set, y_pred)\n",
        "    precision = precision_score(y_set, y_pred)\n",
        "    recall = recall_score(y_set, y_pred)\n",
        "    f1 = f1_score(y_set, y_pred)\n",
        "    print(f\"\\n{set_name} Metrics (Threshold={threshold:.2f}):\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"Classification Report:\\n{classification_report(y_set, y_pred)}\")\n",
        "    return y_scores\n",
        "\n",
        "# Evaluate on validation set and find optimal threshold\n",
        "y_val_scores = evaluate_model_with_threshold(best_model, X_val, y_val, \"Validation\")\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_val, y_val_scores)\n",
        "# Find threshold where precision and recall are both >0.90 (if possible)\n",
        "valid_thresholds = thresholds[(precisions[:-1] > 0.90) & (recalls[:-1] > 0.90)]\n",
        "optimal_threshold = valid_thresholds[0] if len(valid_thresholds) > 0 else 0.5\n",
        "print(f\"Optimal Threshold from Validation: {optimal_threshold:.2f}\")\n",
        "\n",
        "# Step 6: Evaluate all sets with optimal threshold\n",
        "evaluate_model_with_threshold(best_model, X_train, y_train, \"Training\", optimal_threshold)\n",
        "evaluate_model_with_threshold(best_model, X_val, y_val, \"Validation\", optimal_threshold)\n",
        "evaluate_model_with_threshold(best_model, X_test, y_test, \"Test\", optimal_threshold)\n",
        "\n",
        "# Step 7: Feature importance\n",
        "cat_transformer = best_model.named_steps['preprocessor'].named_transformers_['cat']\n",
        "cat_feature_names = cat_transformer.get_feature_names_out(categorical_cols)\n",
        "feature_names = numerical_cols + list(cat_feature_names)\n",
        "importances = best_model.named_steps['classifier'].feature_importances_\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance.sort_values(by='Importance', ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImUwxxRR1IRa",
        "outputId": "0a942b8a-e300-47d4-f206-152a1ace9c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n",
            "Best Parameters: {'classifier__colsample_bytree': 0.7, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 100, 'classifier__scale_pos_weight': 1, 'classifier__subsample': 0.7}\n",
            "\n",
            "Validation Metrics (Threshold=0.50):\n",
            "Accuracy: 0.90\n",
            "Precision: 0.89\n",
            "Recall: 0.89\n",
            "F1-Score: 0.89\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91        23\n",
            "           1       0.89      0.89      0.89        18\n",
            "\n",
            "    accuracy                           0.90        41\n",
            "   macro avg       0.90      0.90      0.90        41\n",
            "weighted avg       0.90      0.90      0.90        41\n",
            "\n",
            "Optimal Threshold from Validation: 0.50\n",
            "\n",
            "Training Metrics (Threshold=0.50):\n",
            "Accuracy: 0.87\n",
            "Precision: 0.87\n",
            "Recall: 0.85\n",
            "F1-Score: 0.86\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.89      0.89       104\n",
            "           1       0.87      0.85      0.86        84\n",
            "\n",
            "    accuracy                           0.87       188\n",
            "   macro avg       0.87      0.87      0.87       188\n",
            "weighted avg       0.87      0.87      0.87       188\n",
            "\n",
            "\n",
            "Validation Metrics (Threshold=0.50):\n",
            "Accuracy: 0.90\n",
            "Precision: 0.89\n",
            "Recall: 0.89\n",
            "F1-Score: 0.89\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91        23\n",
            "           1       0.89      0.89      0.89        18\n",
            "\n",
            "    accuracy                           0.90        41\n",
            "   macro avg       0.90      0.90      0.90        41\n",
            "weighted avg       0.90      0.90      0.90        41\n",
            "\n",
            "\n",
            "Test Metrics (Threshold=0.50):\n",
            "Accuracy: 0.90\n",
            "Precision: 0.85\n",
            "Recall: 0.94\n",
            "F1-Score: 0.89\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.87      0.91        23\n",
            "           1       0.85      0.94      0.89        18\n",
            "\n",
            "    accuracy                           0.90        41\n",
            "   macro avg       0.90      0.91      0.90        41\n",
            "weighted avg       0.91      0.90      0.90        41\n",
            "\n",
            "\n",
            "Feature Importance:\n",
            "                      Feature  Importance\n",
            "19                 Thallium_7    0.150420\n",
            "12          Exercise angina_1    0.134737\n",
            "8           Chest pain type_4    0.104231\n",
            "13              Slope of ST_2    0.071208\n",
            "4               ST depression    0.062554\n",
            "5                       Sex_1    0.056909\n",
            "16  Number of vessels fluro_2    0.052742\n",
            "0                         Age    0.052010\n",
            "3                      Max HR    0.047843\n",
            "15  Number of vessels fluro_1    0.045130\n",
            "14              Slope of ST_3    0.036981\n",
            "11              EKG results_2    0.032286\n",
            "18                 Thallium_6    0.031490\n",
            "2                 Cholesterol    0.030040\n",
            "1                          BP    0.027140\n",
            "7           Chest pain type_3    0.024234\n",
            "6           Chest pain type_2    0.022546\n",
            "9              FBS over 120_1    0.017498\n",
            "10              EKG results_1    0.000000\n",
            "17  Number of vessels fluro_3    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_excel('Heart_Disease_Prediction .xlsx')\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "# Encode target variable\n",
        "df['Heart Disease'] = df['Heart Disease'].map({'Presence': 1, 'Absence': 0})\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop('Heart Disease', axis=1)\n",
        "y = df['Heart Disease']\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['Sex', 'Chest pain type', 'FBS over 120', 'EKG results',\n",
        "                    'Exercise angina', 'Slope of ST', 'Number of vessels fluro', 'Thallium']\n",
        "numerical_cols = ['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression']\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Step 2: Split the data\n",
        "# Train (70%), Validation (15%), Test (15%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Step 3: Create pipeline with SMOTE and XGBoost\n",
        "xgb = XGBClassifier(random_state=42, eval_metric='logloss', scale_pos_weight=1.5)  # Class weight adjusted\n",
        "pipeline = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', xgb)\n",
        "])\n",
        "\n",
        "# Step 4: Expanded hyperparameter tuning with more options\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 150, 200, 300, 500],\n",
        "    'classifier__max_depth': [3, 4, 5, 6],\n",
        "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'classifier__subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    'classifier__colsample_bytree': [0.7, 0.8, 0.9],\n",
        "    'classifier__scale_pos_weight': [1.2, 1.5, 2]  # Adjust for class imbalance\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Step 5: Threshold tuning on validation set\n",
        "def evaluate_model_with_threshold(model, X_set, y_set, set_name, threshold=0.5):\n",
        "    y_scores = model.predict_proba(X_set)[:, 1]\n",
        "    y_pred = (y_scores >= threshold).astype(int)\n",
        "    accuracy = accuracy_score(y_set, y_pred)\n",
        "    precision = precision_score(y_set, y_pred)\n",
        "    recall = recall_score(y_set, y_pred)\n",
        "    f1 = f1_score(y_set, y_pred)\n",
        "    print(f\"\\n{set_name} Metrics (Threshold={threshold:.2f}):\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"Classification Report:\\n{classification_report(y_set, y_pred)}\")\n",
        "    return y_scores\n",
        "\n",
        "# Evaluate on validation set and find optimal threshold\n",
        "y_val_scores = evaluate_model_with_threshold(best_model, X_val, y_val, \"Validation\")\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_val, y_val_scores)\n",
        "# Find threshold where precision and recall are both >0.90 (if possible)\n",
        "valid_thresholds = thresholds[(precisions[:-1] > 0.90) & (recalls[:-1] > 0.90)]\n",
        "optimal_threshold = valid_thresholds[0] if len(valid_thresholds) > 0 else 0.5\n",
        "print(f\"Optimal Threshold from Validation: {optimal_threshold:.2f}\")\n",
        "\n",
        "# Step 6: Evaluate all sets with optimal threshold\n",
        "evaluate_model_with_threshold(best_model, X_train, y_train, \"Training\", optimal_threshold)\n",
        "evaluate_model_with_threshold(best_model, X_val, y_val, \"Validation\", optimal_threshold)\n",
        "evaluate_model_with_threshold(best_model, X_test, y_test, \"Test\", optimal_threshold)\n",
        "\n",
        "# Step 7: Feature importance\n",
        "cat_transformer = best_model.named_steps['preprocessor'].named_transformers_['cat']\n",
        "cat_feature_names = cat_transformer.get_feature_names_out(categorical_cols)\n",
        "feature_names = numerical_cols + list(cat_feature_names)\n",
        "importances = best_model.named_steps['classifier'].feature_importances_\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance.sort_values(by='Importance', ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivB3PxDa3VQy",
        "outputId": "a09dbba2-e1bb-4a7f-881a-465c3bbcd52d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 2880 candidates, totalling 14400 fits\n",
            "Best Parameters: {'classifier__colsample_bytree': 0.7, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 100, 'classifier__scale_pos_weight': 1.2, 'classifier__subsample': 0.7}\n",
            "\n",
            "Validation Metrics (Threshold=0.50):\n",
            "Accuracy: 0.90\n",
            "Precision: 0.89\n",
            "Recall: 0.89\n",
            "F1-Score: 0.89\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91        23\n",
            "           1       0.89      0.89      0.89        18\n",
            "\n",
            "    accuracy                           0.90        41\n",
            "   macro avg       0.90      0.90      0.90        41\n",
            "weighted avg       0.90      0.90      0.90        41\n",
            "\n",
            "Optimal Threshold from Validation: 0.50\n",
            "\n",
            "Training Metrics (Threshold=0.50):\n",
            "Accuracy: 0.87\n",
            "Precision: 0.83\n",
            "Recall: 0.90\n",
            "F1-Score: 0.86\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.85      0.88       104\n",
            "           1       0.83      0.90      0.86        84\n",
            "\n",
            "    accuracy                           0.87       188\n",
            "   macro avg       0.87      0.88      0.87       188\n",
            "weighted avg       0.88      0.87      0.87       188\n",
            "\n",
            "\n",
            "Validation Metrics (Threshold=0.50):\n",
            "Accuracy: 0.90\n",
            "Precision: 0.89\n",
            "Recall: 0.89\n",
            "F1-Score: 0.89\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91        23\n",
            "           1       0.89      0.89      0.89        18\n",
            "\n",
            "    accuracy                           0.90        41\n",
            "   macro avg       0.90      0.90      0.90        41\n",
            "weighted avg       0.90      0.90      0.90        41\n",
            "\n",
            "\n",
            "Test Metrics (Threshold=0.50):\n",
            "Accuracy: 0.85\n",
            "Precision: 0.77\n",
            "Recall: 0.94\n",
            "F1-Score: 0.85\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.78      0.86        23\n",
            "           1       0.77      0.94      0.85        18\n",
            "\n",
            "    accuracy                           0.85        41\n",
            "   macro avg       0.86      0.86      0.85        41\n",
            "weighted avg       0.87      0.85      0.85        41\n",
            "\n",
            "\n",
            "Feature Importance:\n",
            "                      Feature  Importance\n",
            "19                 Thallium_7    0.152402\n",
            "12          Exercise angina_1    0.113117\n",
            "8           Chest pain type_4    0.104687\n",
            "13              Slope of ST_2    0.073925\n",
            "4               ST depression    0.060999\n",
            "5                       Sex_1    0.056753\n",
            "0                         Age    0.051833\n",
            "3                      Max HR    0.048783\n",
            "17  Number of vessels fluro_3    0.047611\n",
            "16  Number of vessels fluro_2    0.047140\n",
            "15  Number of vessels fluro_1    0.041816\n",
            "2                 Cholesterol    0.031938\n",
            "18                 Thallium_6    0.030191\n",
            "11              EKG results_2    0.029848\n",
            "14              Slope of ST_3    0.026138\n",
            "1                          BP    0.025176\n",
            "9              FBS over 120_1    0.023965\n",
            "7           Chest pain type_3    0.022262\n",
            "6           Chest pain type_2    0.011417\n",
            "10              EKG results_1    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Print TensorFlow version and GPU availability\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {tf.test.is_gpu_available()}\")\n",
        "\n",
        "# Load the dataset (already uploaded in Colab)\n",
        "df = pd.read_excel('Heart_Disease_Prediction .xlsx')\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "# Encode target variable\n",
        "df['Heart Disease'] = df['Heart Disease'].map({'Presence': 1, 'Absence': 0})\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop('Heart Disease', axis=1)\n",
        "y = df['Heart Disease']\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['Sex', 'Chest pain type', 'FBS over 120', 'EKG results',\n",
        "                    'Exercise angina', 'Slope of ST', 'Number of vessels fluro', 'Thallium']\n",
        "numerical_cols = ['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression']\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Step 2: Split the data\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Apply preprocessing\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_val = preprocessor.transform(X_val)\n",
        "X_test = preprocessor.transform(X_test)\n",
        "\n",
        "# Step 3: Compute class weights for imbalance\n",
        "class_counts = np.bincount(y_train)\n",
        "class_weight = {0: 1.0, 1: class_counts[0] / class_counts[1]}  # ~1.24 for 104:84 split\n",
        "\n",
        "# Step 4: Build deep learning model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Step 5: Train model with error handling\n",
        "try:\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=100,\n",
        "        batch_size=16,\n",
        "        class_weight=class_weight,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Training failed: {e}\")\n",
        "    print(\"Retrying with minimal configuration...\")\n",
        "    # Rebuild and compile model to reset state\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        Dropout(0.3),\n",
        "        Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=1000,\n",
        "        batch_size=16,\n",
        "        class_weight=class_weight,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "# Step 6: Threshold tuning on validation set\n",
        "def evaluate_model_with_threshold(model, X_set, y_set, set_name, threshold=0.5):\n",
        "    y_scores = model.predict(X_set, verbose=0).flatten()\n",
        "    y_pred = (y_scores >= threshold).astype(int)\n",
        "    accuracy = accuracy_score(y_set, y_pred)\n",
        "    precision = precision_score(y_set, y_pred)\n",
        "    recall = recall_score(y_set, y_pred)\n",
        "    f1 = f1_score(y_set, y_pred)\n",
        "    print(f\"\\n{set_name} Metrics (Threshold={threshold:.2f}):\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"Classification Report:\\n{classification_report(y_set, y_pred)}\")\n",
        "    return y_scores\n",
        "\n",
        "# Evaluate on validation set and find optimal threshold\n",
        "y_val_scores = evaluate_model_with_threshold(model, X_val, y_val, \"Validation\")\n",
        "thresholds = np.arange(0.3, 0.71, 0.01)\n",
        "best_threshold = 0.5\n",
        "best_f1 = 0\n",
        "for t in thresholds:\n",
        "    y_pred = (y_val_scores >= t).astype(int)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred)\n",
        "    recall = recall_score(y_val, y_pred)\n",
        "    if precision >= 0.90 and recall >= 0.90 and f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_threshold = t\n",
        "\n",
        "print(f\"Optimal Threshold from Validation: {best_threshold:.2f}\")\n",
        "\n",
        "# Step 7: Evaluate all sets with optimal threshold\n",
        "evaluate_model_with_threshold(model, X_train, y_train, \"Training\", best_threshold)\n",
        "evaluate_model_with_threshold(model, X_val, y_val, \"Validation\", best_threshold)\n",
        "evaluate_model_with_threshold(model, X_test, y_test, \"Test\", best_threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dla0HRTO7LeG",
        "outputId": "0ba67869-0db2-44a1-cc5b-f29f21a6f925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.18.0\n",
            "GPU Available: True\n",
            "Epoch 1/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 193ms/step - accuracy: 0.4046 - loss: 1.7489 - val_accuracy: 0.5854 - val_loss: 1.5738\n",
            "Epoch 2/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4306 - loss: 1.6402 - val_accuracy: 0.6829 - val_loss: 1.4917\n",
            "Epoch 3/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4777 - loss: 1.5737 - val_accuracy: 0.7561 - val_loss: 1.4152\n",
            "Epoch 4/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6371 - loss: 1.4847 - val_accuracy: 0.7805 - val_loss: 1.3401\n",
            "Epoch 5/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6391 - loss: 1.4035 - val_accuracy: 0.8049 - val_loss: 1.2665\n",
            "Epoch 6/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6188 - loss: 1.3696 - val_accuracy: 0.8049 - val_loss: 1.1983\n",
            "Epoch 7/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7343 - loss: 1.2855 - val_accuracy: 0.8537 - val_loss: 1.1336\n",
            "Epoch 8/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6840 - loss: 1.2669 - val_accuracy: 0.8537 - val_loss: 1.0760\n",
            "Epoch 9/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6975 - loss: 1.1866 - val_accuracy: 0.8537 - val_loss: 1.0231\n",
            "Epoch 10/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6944 - loss: 1.1446 - val_accuracy: 0.8537 - val_loss: 0.9706\n",
            "Epoch 11/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7720 - loss: 1.0640 - val_accuracy: 0.8537 - val_loss: 0.9251\n",
            "Epoch 12/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7605 - loss: 1.0396 - val_accuracy: 0.8780 - val_loss: 0.8865\n",
            "Epoch 13/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8104 - loss: 1.0089 - val_accuracy: 0.8537 - val_loss: 0.8539\n",
            "Epoch 14/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8651 - loss: 0.9080 - val_accuracy: 0.8780 - val_loss: 0.8209\n",
            "Epoch 15/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8149 - loss: 0.9160 - val_accuracy: 0.8537 - val_loss: 0.7911\n",
            "Epoch 16/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8162 - loss: 0.9096 - val_accuracy: 0.8537 - val_loss: 0.7632\n",
            "Epoch 17/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7991 - loss: 0.8949 - val_accuracy: 0.8780 - val_loss: 0.7435\n",
            "Epoch 18/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7784 - loss: 0.8596 - val_accuracy: 0.8537 - val_loss: 0.7194\n",
            "Epoch 19/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8515 - loss: 0.7802 - val_accuracy: 0.8780 - val_loss: 0.6997\n",
            "Epoch 20/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8263 - loss: 0.7590 - val_accuracy: 0.8780 - val_loss: 0.6868\n",
            "Epoch 21/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8406 - loss: 0.7886 - val_accuracy: 0.8780 - val_loss: 0.6665\n",
            "Epoch 22/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8554 - loss: 0.7278 - val_accuracy: 0.8780 - val_loss: 0.6567\n",
            "Epoch 23/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8626 - loss: 0.6948 - val_accuracy: 0.8780 - val_loss: 0.6425\n",
            "Epoch 24/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8541 - loss: 0.7150 - val_accuracy: 0.8780 - val_loss: 0.6326\n",
            "Epoch 25/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8148 - loss: 0.7013 - val_accuracy: 0.8780 - val_loss: 0.6209\n",
            "Epoch 26/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8461 - loss: 0.6691 - val_accuracy: 0.8780 - val_loss: 0.6164\n",
            "Epoch 27/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8781 - loss: 0.6202 - val_accuracy: 0.8780 - val_loss: 0.6025\n",
            "Epoch 28/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8558 - loss: 0.6472 - val_accuracy: 0.8780 - val_loss: 0.5892\n",
            "Epoch 29/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8610 - loss: 0.6556 - val_accuracy: 0.8780 - val_loss: 0.5934\n",
            "Epoch 30/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8514 - loss: 0.6371 - val_accuracy: 0.8780 - val_loss: 0.5910\n",
            "Epoch 31/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8784 - loss: 0.6122 - val_accuracy: 0.8780 - val_loss: 0.5771\n",
            "Epoch 32/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8911 - loss: 0.5592 - val_accuracy: 0.8780 - val_loss: 0.5728\n",
            "Epoch 33/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8505 - loss: 0.6102 - val_accuracy: 0.8780 - val_loss: 0.5701\n",
            "Epoch 34/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8971 - loss: 0.5733 - val_accuracy: 0.8780 - val_loss: 0.5603\n",
            "Epoch 35/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8912 - loss: 0.5759 - val_accuracy: 0.8780 - val_loss: 0.5585\n",
            "Epoch 36/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8741 - loss: 0.5799 - val_accuracy: 0.8780 - val_loss: 0.5549\n",
            "Epoch 37/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8516 - loss: 0.5895 - val_accuracy: 0.8780 - val_loss: 0.5508\n",
            "Epoch 38/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8589 - loss: 0.5943 - val_accuracy: 0.8780 - val_loss: 0.5493\n",
            "Epoch 39/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9082 - loss: 0.5471 - val_accuracy: 0.8780 - val_loss: 0.5608\n",
            "Epoch 40/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8882 - loss: 0.5511 - val_accuracy: 0.8780 - val_loss: 0.5510\n",
            "Epoch 41/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8728 - loss: 0.5915 - val_accuracy: 0.8780 - val_loss: 0.5486\n",
            "Epoch 42/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8830 - loss: 0.5811 - val_accuracy: 0.9024 - val_loss: 0.5515\n",
            "Epoch 43/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9135 - loss: 0.5171 - val_accuracy: 0.8780 - val_loss: 0.5525\n",
            "Epoch 44/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8875 - loss: 0.5386 - val_accuracy: 0.8780 - val_loss: 0.5306\n",
            "Epoch 45/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9204 - loss: 0.5105 - val_accuracy: 0.8780 - val_loss: 0.5575\n",
            "Epoch 46/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9360 - loss: 0.4514 - val_accuracy: 0.9024 - val_loss: 0.5592\n",
            "Epoch 47/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9047 - loss: 0.4916 - val_accuracy: 0.8780 - val_loss: 0.5249\n",
            "Epoch 48/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8650 - loss: 0.5560 - val_accuracy: 0.8780 - val_loss: 0.5399\n",
            "Epoch 49/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9258 - loss: 0.5154 - val_accuracy: 0.9024 - val_loss: 0.5734\n",
            "Epoch 50/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8966 - loss: 0.5336 - val_accuracy: 0.8780 - val_loss: 0.5460\n",
            "Epoch 51/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8603 - loss: 0.5563 - val_accuracy: 0.8780 - val_loss: 0.5475\n",
            "Epoch 52/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8864 - loss: 0.5194 - val_accuracy: 0.9024 - val_loss: 0.5500\n",
            "Epoch 53/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8902 - loss: 0.4772 - val_accuracy: 0.9024 - val_loss: 0.5484\n",
            "Epoch 54/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9380 - loss: 0.4682 - val_accuracy: 0.9024 - val_loss: 0.5484\n",
            "Epoch 55/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9049 - loss: 0.4553 - val_accuracy: 0.9024 - val_loss: 0.5432\n",
            "Epoch 56/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9270 - loss: 0.4207 - val_accuracy: 0.9024 - val_loss: 0.5457\n",
            "Epoch 57/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9188 - loss: 0.4642 - val_accuracy: 0.9024 - val_loss: 0.5323\n",
            "\n",
            "Validation Metrics (Threshold=0.50):\n",
            "Accuracy: 0.88\n",
            "Precision: 0.84\n",
            "Recall: 0.89\n",
            "F1-Score: 0.86\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.87      0.89        23\n",
            "           1       0.84      0.89      0.86        18\n",
            "\n",
            "    accuracy                           0.88        41\n",
            "   macro avg       0.88      0.88      0.88        41\n",
            "weighted avg       0.88      0.88      0.88        41\n",
            "\n",
            "Optimal Threshold from Validation: 0.50\n",
            "\n",
            "Training Metrics (Threshold=0.50):\n",
            "Accuracy: 0.94\n",
            "Precision: 0.97\n",
            "Recall: 0.89\n",
            "F1-Score: 0.93\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95       104\n",
            "           1       0.97      0.89      0.93        84\n",
            "\n",
            "    accuracy                           0.94       188\n",
            "   macro avg       0.95      0.94      0.94       188\n",
            "weighted avg       0.94      0.94      0.94       188\n",
            "\n",
            "\n",
            "Validation Metrics (Threshold=0.50):\n",
            "Accuracy: 0.88\n",
            "Precision: 0.84\n",
            "Recall: 0.89\n",
            "F1-Score: 0.86\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.87      0.89        23\n",
            "           1       0.84      0.89      0.86        18\n",
            "\n",
            "    accuracy                           0.88        41\n",
            "   macro avg       0.88      0.88      0.88        41\n",
            "weighted avg       0.88      0.88      0.88        41\n",
            "\n",
            "\n",
            "Test Metrics (Threshold=0.50):\n",
            "Accuracy: 0.88\n",
            "Precision: 0.81\n",
            "Recall: 0.94\n",
            "F1-Score: 0.87\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.83      0.88        23\n",
            "           1       0.81      0.94      0.87        18\n",
            "\n",
            "    accuracy                           0.88        41\n",
            "   macro avg       0.88      0.89      0.88        41\n",
            "weighted avg       0.89      0.88      0.88        41\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.1446545 , 0.7849375 , 0.13904195, 0.67620116, 0.07238589,\n",
              "       0.8652813 , 0.15106726, 0.69651127, 0.22773719, 0.9804182 ,\n",
              "       0.99428   , 0.1473177 , 0.3044472 , 0.86931247, 0.9683708 ,\n",
              "       0.8390678 , 0.88249767, 0.95584846, 0.955394  , 0.95700693,\n",
              "       0.27728915, 0.94511974, 0.948141  , 0.9438954 , 0.01757617,\n",
              "       0.02902043, 0.16862495, 0.92657614, 0.10458346, 0.975963  ,\n",
              "       0.131153  , 0.16245401, 0.07308483, 0.9494881 , 0.99004   ,\n",
              "       0.21059312, 0.06299305, 0.09785069, 0.09997994, 0.9874676 ,\n",
              "       0.10108708], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Load data from Excel file\n",
        "file_path = 'Heart_Disease_Prediction .xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Map the target labels if needed\n",
        "data['Heart Disease'] = data['Heart Disease'].replace({0: 'Absence', 1: 'Presence'})\n",
        "\n",
        "# Set features and target variable\n",
        "X = data.drop('Heart Disease', axis=1)  # Features\n",
        "y = data['Heart Disease']  # Target variable\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Select top K best features using SelectKBest\n",
        "selector = SelectKBest(f_classif, k=min(8, X_train_scaled.shape[1]))  # Max 8 features\n",
        "X_train_scaled = selector.fit_transform(X_train_scaled, y_train)\n",
        "X_val_scaled = selector.transform(X_val_scaled)\n",
        "X_test_scaled = selector.transform(X_test_scaled)\n",
        "\n",
        "# Now the data is ready for model training\n",
        "\n",
        "# Define different models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning for Random Forest and Gradient Boosting using GridSearchCV\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "param_grid_gb = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "# Train models and evaluate them\n",
        "best_model = None\n",
        "best_score = 0\n",
        "best_model_name = \"\"\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "\n",
        "    if model_name == 'Random Forest':\n",
        "        grid_search = GridSearchCV(model, param_grid_rf, cv=5, scoring='accuracy')\n",
        "        grid_search.fit(X_train_scaled, y_train)\n",
        "        model = grid_search.best_estimator_\n",
        "\n",
        "    elif model_name == 'Gradient Boosting':\n",
        "        grid_search = GridSearchCV(model, param_grid_gb, cv=5, scoring='accuracy')\n",
        "        grid_search.fit(X_train_scaled, y_train)\n",
        "        model = grid_search.best_estimator_\n",
        "\n",
        "    # Train the final model\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predict on validation data\n",
        "    y_pred_val = model.predict(X_val_scaled)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_val, y_pred_val)\n",
        "    precision = precision_score(y_val, y_pred_val, pos_label='Presence')  # Use 'Presence' as pos_label\n",
        "    recall = recall_score(y_val, y_pred_val, pos_label='Presence')        # Use 'Presence' as pos_label\n",
        "    f1 = f1_score(y_val, y_pred_val, pos_label='Presence')                # Use 'Presence' as pos_label\n",
        "\n",
        "    print(f\"\\n{model_name} Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    if accuracy > best_score:\n",
        "        best_score = accuracy\n",
        "        best_model = model\n",
        "        best_model_name = model_name\n",
        "\n",
        "# Predict on final test data using the best model\n",
        "y_pred_test = best_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the best model on test data\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "test_precision = precision_score(y_test, y_pred_test, pos_label='Presence')\n",
        "test_recall = recall_score(y_test, y_pred_test, pos_label='Presence')\n",
        "test_f1 = f1_score(y_test, y_pred_test, pos_label='Presence')\n",
        "\n",
        "print(f\"\\nBest Model: {best_model_name} with Accuracy: {best_score:.4f}\")\n",
        "print(\"\\nTest Metrics:\")\n",
        "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall: {test_recall:.4f}\")\n",
        "print(f\"F1 Score: {test_f1:.4f}\")\n",
        "\n",
        "# Classification report for detailed evaluation\n",
        "print(\"\\nClassification Report for Test Data:\")\n",
        "print(classification_report(y_test, y_pred_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZHO2ZLItZ_y",
        "outputId": "40d0a931-860f-4ece-a744-ae8bef47152e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Logistic Regression...\n",
            "\n",
            "Logistic Regression Metrics:\n",
            "Accuracy: 0.7750\n",
            "Precision: 0.7000\n",
            "Recall: 0.5385\n",
            "F1 Score: 0.6087\n",
            "\n",
            "Training Random Forest...\n",
            "\n",
            "Random Forest Metrics:\n",
            "Accuracy: 0.8000\n",
            "Precision: 0.8571\n",
            "Recall: 0.4615\n",
            "F1 Score: 0.6000\n",
            "\n",
            "Training Gradient Boosting...\n",
            "\n",
            "Gradient Boosting Metrics:\n",
            "Accuracy: 0.8250\n",
            "Precision: 0.8000\n",
            "Recall: 0.6154\n",
            "F1 Score: 0.6957\n",
            "\n",
            "Best Model: Gradient Boosting with Accuracy: 0.8250\n",
            "\n",
            "Test Metrics:\n",
            "Accuracy: 0.9024\n",
            "Precision: 1.0000\n",
            "Recall: 0.7895\n",
            "F1 Score: 0.8824\n",
            "\n",
            "Classification Report for Test Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Absence       0.85      1.00      0.92        22\n",
            "    Presence       1.00      0.79      0.88        19\n",
            "\n",
            "    accuracy                           0.90        41\n",
            "   macro avg       0.92      0.89      0.90        41\n",
            "weighted avg       0.92      0.90      0.90        41\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform predictions after pipeline preprocessing (including scaling and one-hot encoding)\n",
        "rf_pred_test = pipeline.set_params(classifier=rf_model).predict(X_test)\n",
        "xgb_pred_test = pipeline.set_params(classifier=xgb_model).predict(X_test)\n",
        "\n",
        "# Create confusion matrix for McNemar's test\n",
        "cm = confusion_matrix(rf_pred_test, xgb_pred_test)\n",
        "\n",
        "# Perform McNemar's test\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "result = mcnemar(cm, exact=True)\n",
        "\n",
        "# Print the results\n",
        "print(f\"\\nMcNemar Test Statistic: {result.statistic}\")\n",
        "print(f\"P-value: {result.pvalue}\")\n",
        "\n",
        "# Interpretation of McNemar's Test\n",
        "if result.pvalue < 0.05:\n",
        "    print(\"The difference between the two models is statistically significant.\")\n",
        "else:\n",
        "    print(\"The difference between the two models is not statistically significant.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jhl5AVD3DMZ",
        "outputId": "251023d0-c308-4d53-dc26-b764f093e41b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "McNemar Test Statistic: 0.0\n",
            "P-value: 1.0\n",
            "The difference between the two models is not statistically significant.\n"
          ]
        }
      ]
    }
  ]
}